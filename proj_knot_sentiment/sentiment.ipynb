{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0b1a776",
      "metadata": {
        "id": "a0b1a776"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dookda/cmu_py499/blob/main/proj_knot_sentiment/sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bbb96ed",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "2bbb96ed"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn\n",
        "!pip install torch torchvision         # for PyTorch\n",
        "!pip install tensorflow                # for TF/Keras (if preferred)\n",
        "!pip install transformers              # for BERT\n",
        "!pip install nltk regex emoji          # for preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CeAlPdx970K",
        "outputId": "b072b7de-30b7-4946-e9b0-7b6993e67738"
      },
      "id": "6CeAlPdx970K",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!cp /content/drive/MyDrive/_DATASET/sentiment/tweets.csv -d /content/tweets.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcNIMLQM-mLP",
        "outputId": "b10c6409-1fe2-4cee-8f6a-245320c75f12"
      },
      "id": "KcNIMLQM-mLP",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('tweets.csv')\n",
        "print(df.airline_sentiment.value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5LrC9J8-SZo",
        "outputId": "21419ab2-4892-4660-bace-04c46f26f383"
      },
      "id": "K5LrC9J8-SZo",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "airline_sentiment\n",
            "negative    9178\n",
            "neutral     3099\n",
            "positive    2363\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, emoji\n",
        "from nltk.corpus import stopwords\n",
        "STOP = set(stopwords.words('english'))\n",
        "\n",
        "def clean_tweet(s):\n",
        "    s = emoji.demojize(s)                            # 😊 → :smiling_face:\n",
        "    s = re.sub(r'http\\S+|@\\w+|#','', s.lower())      # remove URLs, mentions, hashtags\n",
        "    s = re.sub(r'[^a-z0-9_: ]',' ', s)               # keep emojis codes & ascii\n",
        "    tokens = [t for t in s.split() if t not in STOP]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_text'] = df.text.apply(clean_tweet)\n"
      ],
      "metadata": {
        "id": "zdVb_vi4_onA"
      },
      "id": "zdVb_vi4_onA",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df.airline_sentiment, random_state=42)\n"
      ],
      "metadata": {
        "id": "KpffXvcFAOhy"
      },
      "id": "KpffXvcFAOhy",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import emoji\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
        "\n",
        "# 1. Download NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 2. Load dataset\n",
        "df = pd.read_csv('tweets.csv')[['text','airline_sentiment']].dropna()\n",
        "\n",
        "# 3. Clean text\n",
        "STOP = set(stopwords.words('english'))\n",
        "def clean_text(s):\n",
        "    s = emoji.demojize(s)                              # 😊 → :smiling_face:\n",
        "    s = re.sub(r'http\\S+|@\\w+|#', '', s.lower())       # remove URLs, mentions, hashtags\n",
        "    s = re.sub(r'[^a-z0-9_: ]', ' ', s)                # keep emoji codes & alphanum\n",
        "    tokens = [t for t in s.split() if t not in STOP]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_text'] = df['text'].map(clean_text)\n",
        "\n",
        "# 4. Encode labels\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['airline_sentiment'])\n",
        "\n",
        "# 5. Train/validation split\n",
        "train_df, val_df = train_test_split(\n",
        "    df[['clean_text','label']],\n",
        "    test_size=0.2,\n",
        "    stratify=df['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 6. Tokenize & pad\n",
        "MAX_LEN = 50\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['clean_text'])\n",
        "\n",
        "def to_padded(seqs):\n",
        "    s = tokenizer.texts_to_sequences(seqs)\n",
        "    return pad_sequences(s, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "X_train, X_val = to_padded(train_df['clean_text']), to_padded(val_df['clean_text'])\n",
        "y_train, y_val = train_df['label'].values, val_df['label'].values\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "EMB_DIM    = 100\n",
        "\n",
        "# 7. Ensure GloVe file exists (download & unzip if needed)\n",
        "GLOVE_FILE = 'glove.6B.100d.txt'\n",
        "if not os.path.isfile(GLOVE_FILE):\n",
        "    print(\"Downloading GloVe embeddings (862 MB zip)...\")\n",
        "    url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "    resp = urllib.request.urlopen(url)\n",
        "    with zipfile.ZipFile(io.BytesIO(resp.read())) as z:\n",
        "        print(\"Extracting GloVe 100d vectors...\")\n",
        "        z.extract(GLOVE_FILE)\n",
        "    print(\"Done.\")\n",
        "\n",
        "# 8. Load GloVe into a dict\n",
        "embeddings_index = {}\n",
        "with open(GLOVE_FILE, 'r', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word   = values[0]\n",
        "        vec    = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = vec\n",
        "\n",
        "# 9. Build embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, EMB_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    vec = embeddings_index.get(word)\n",
        "    if vec is not None:\n",
        "        embedding_matrix[i] = vec\n",
        "\n",
        "# 10. Define the Bi-LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=EMB_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_LEN,\n",
        "        trainable=False\n",
        "    ),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(le.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "# 11. Train\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "# 12. Evaluate\n",
        "y_pred = model.predict(X_val).argmax(axis=-1)\n",
        "print(classification_report(\n",
        "    y_val, y_pred,\n",
        "    target_names=le.classes_, digits=4\n",
        "))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "id": "oZTThoL-AUxh",
        "outputId": "8d1f7a51-35ea-4baa-c306-67e6c19f51f1"
      },
      "id": "oZTThoL-AUxh",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings (862 MB zip)...\n",
            "Extracting GloVe 100d vectors...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m1,156,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,156,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,156,000\u001b[0m (4.41 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,156,000</span> (4.41 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,156,000\u001b[0m (4.41 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,156,000</span> (4.41 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 102ms/step - accuracy: 0.6477 - loss: 0.8065 - val_accuracy: 0.7428 - val_loss: 0.6198\n",
            "Epoch 2/5\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 89ms/step - accuracy: 0.7511 - loss: 0.6068 - val_accuracy: 0.7616 - val_loss: 0.5961\n",
            "Epoch 3/5\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 85ms/step - accuracy: 0.7704 - loss: 0.5705 - val_accuracy: 0.7650 - val_loss: 0.5762\n",
            "Epoch 4/5\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 89ms/step - accuracy: 0.7806 - loss: 0.5471 - val_accuracy: 0.7749 - val_loss: 0.5570\n",
            "Epoch 5/5\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 85ms/step - accuracy: 0.7920 - loss: 0.5177 - val_accuracy: 0.7678 - val_loss: 0.5658\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.8440    0.8578    0.8508      1835\n",
            "     neutral     0.5756    0.5774    0.5765       620\n",
            "    positive     0.7166    0.6681    0.6915       473\n",
            "\n",
            "    accuracy                         0.7678      2928\n",
            "   macro avg     0.7120    0.7011    0.7063      2928\n",
            "weighted avg     0.7666    0.7678    0.7670      2928\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Test on a new tweet\n",
        "def predict_sentiment(tweet):\n",
        "    tweet_clean = clean_text(tweet)\n",
        "    seq = tokenizer.texts_to_sequences([tweet_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "    pred = model.predict(padded_seq).argmax(axis=-1)\n",
        "    return le.inverse_transform(pred)[0]\n",
        "# Example usage\n",
        "tweet = \"I love flying with this airline! 😊✈️\"\n",
        "\n",
        "print(f\"Sentiment: {predict_sentiment(tweet)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQeJy4nDCvcA",
        "outputId": "eac31c5a-1120-4d35-b246-ef9d7e05f906"
      },
      "id": "xQeJy4nDCvcA",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "Sentiment: positive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}